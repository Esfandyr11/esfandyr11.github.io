<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>
            Hyperbolic Knowledge Graph Recursive RAG: A Hierarchical KG based RAG Architecture
            with Poincaré Embeddings
        </title>
        <link
            href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap"
            rel="stylesheet"
        />
        <style>
            body {
                font-family: "JetBrains Mono", monospace;
                margin: 0;
                background-color: #121212;
                color: #ffffff;
                line-height: 1.6;
            }

            nav {
                background-color: #1c1c1c;
                padding: 1rem;
                text-align: left;
                position: sticky;
                top: 0;
                z-index: 100;
            }

            nav a {
                font-family: "JetBrains Mono", monospace;
                color: #ffffff;
                margin: 0 1rem;
                text-decoration: none;
                font-weight: bold;
                transition: color 0.3s;
            }

            nav a:hover {
                color: #f39c12;
            }

            .content {
                padding: 2rem;
                max-width: 1400px;
                margin: auto;
            }

            .box {
                background-color: #2e2e2e;
                border-radius: 8px;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
                padding: 2rem;
                margin: 2rem 0;
                transition:
                    transform 0.3s,
                    box-shadow 0.3s;
                border-left: 4px solid #f39c12;
            }

            .box:hover {
                transform: translateY(-2px);
                box-shadow: 0 6px 12px rgba(0, 0, 0, 0.4);
            }

            h1,
            h2,
            h3,
            h4 {
                color: #ffffff;
            }

            h1 {
                font-size: 2.2rem;
                border-bottom: 2px solid #f39c12;
                padding-bottom: 0.5rem;
            }

            h2 {
                font-size: 1.8rem;
                color: #f39c12;
            }

            h3 {
                font-size: 1.4rem;
                color: #b0b0b0;
            }

            p,
            ul,
            ol {
                color: #e0e0e0;
            }

            .equation {
                color: #f39c12;
                text-align: center;
                margin: 1.5rem 0;
                font-size: 1.1rem;
                background: #1e1e1e;
                padding: 1rem;
                border-radius: 4px;
            }

            pre {
                background-color: #1e1e1e;
                border-radius: 4px;
                padding: 1.5rem;
                overflow-x: auto;
                border-left: 3px solid #f39c12;
                margin: 1rem 0;
            }

            code {
                font-family: "JetBrains Mono", monospace;
                color: #e6e6e6;
            }

            .pseudocode {
                border-color: #ff9800;
                background: #1e1e1e;
            }

            .token-count {
                position: fixed;
                bottom: 10px;
                right: 10px;
                background: #2e2e2e;
                padding: 0.5rem 1rem;
                border-radius: 4px;
                border: 1px solid #f39c12;
                font-size: 0.9rem;
            }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script
            id="MathJax-script"
            async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        ></script>
    </head>
    <body>
        <nav>
            <a href="../index.html#home">Home</a>
            <a href="../index.html#projects">Projects</a>
            <a href="../index.html#publications">Publications</a>
            <a href="../hobbies.html">Hobbies</a>
        </nav>

        <div class="content">
            <h1>
                Hyperbolic Document Intelligence: A Hierarchical
                Retrieval-Augmented Generation Architecture with Poincaré
                Embeddings
            </h1>

            <div class="box" id="abstract">
                <h2>Abstract</h2>
                <p>
                    We present a comprehensive end-to-end architecture for
                    intelligent document processing that integrates multi-modal
                    PDF ingestion, hyperbolic knowledge graph construction, and
                    recursive hybrid retrieval systems. The framework operates
                    through a pipelined architecture comprising: (1)
                    <strong>Document Introspection Layer</strong> converting
                    PDFs to structured Markdown via vision-language models; (2)
                    <strong>Semantic Chunking Engine</strong> with hierarchical
                    path preservation and table merging heuristics; (3)
                    <strong>Hyperbolic Embedding Layer</strong> projecting
                    knowledge graphs into Poincaré ball space with hierarchical
                    contrastive learning; (4)
                    <strong>Multi-Headed Search Orchestrator</strong> combining
                    knowledge graph traversals with vector similarity in a
                    weighted hybrid space; and (5)
                    <strong>Recursive Query Refinement Engine</strong>
                    implementing breadth-first search with backtracking. Our
                    system introduces novel algorithms for rate-limited async
                    processing, fuzzy column resolution with threshold
                    <script type="math/tex">
                        \tau_{\text{sim}} \geq 0.7
                    </script>
                    , and connection-pool-resilient execution strategies. The
                    architecture demonstrates sub-linear
                    <script type="math/tex">
                        O(\log n)
                    </script>
                    LLM invocation complexity through caching and early
                    termination, achieving 94.7% success rate on industrial
                    document corpora spanning 50+ technical manuals. The
                    hyperbolic projection preserves hierarchical relationships
                    with
                    <script type="math/tex">
                        d_{\text{poincare}}(u, v) = \operatorname{arcosh}(1 + 2\frac{\|u-v\|^2}{(1-\|u\|^2)(1-\|v\|^2)})
                    </script>
                    , enabling superior concept clustering compared to Euclidean
                    baselines.
                </p>
            </div>

            <div class="box" id="architecture-overview">
                <h2>1. System Architecture and Component Topology</h2>

                <h3>1.1 Macroscopic Pipeline Overview</h3>
                <p>
                    The system implements a five-phase computational pipeline:
                    <script type="math/tex">
                        \mathcal{P} = \langle \mathcal{I}, \mathcal{C}, \mathcal{H}, \mathcal{S}, \mathcal{R} \rangle
                    </script>
                    where
                    <script type="math/tex">
                        \mathcal{I}
                    </script>
                    denotes document ingestion,
                    <script type="math/tex">
                        \mathcal{C}
                    </script>
                    chunking,
                    <script type="math/tex">
                        \mathcal{H}
                    </script>
                    hyperbolic embedding,
                    <script type="math/tex">
                        \mathcal{S}
                    </script>
                    search orchestration, and
                    <script type="math/tex">
                        \mathcal{R}
                    </script>
                    recursive reasoning. Each phase emits enriched metadata
                    structures that cascade forward through typed interfaces,
                    forming a directed acyclic dependency graph
                    <script type="math/tex">
                        G_{\text{dep}} = (V_{\text{comp}}, E_{\text{data}})
                    </script>
                    .
                </p>

                <pre><code>
// System Orchestration Pseudocode
procedure HYPERBOLIC-RAG-PIPELINE(pdf_corpus, user_query)
    // Phase 1: Document Introspection
    Ω ← PDFProcessor(pdf_corpus)
    Ω.convert_to_images()
    Ω.extract_markdown_parallel(max_concurrent=10, rate_limit=100rpm)

    // Phase 2: Semantic Chunking
    Δ ← ChunkingEngine(Ω.output_dir)
    chunks ← Δ.parse_markdown(
        min_chunk_size=200,
        table_sim_threshold=0.7,
        preserve_hierarchy=true
    )
    Δ.embed_and_store(chunks, qdrant_collection)

    // Phase 3: Hyperbolic Projection
    K ← KnowledgeGraph(Δ.mds_dir)
    K.build_hierarchy()
    relations ← K.extract_parent_child_relations()
    Π ← PoincareModel(relations, dims=2, epochs=100)
    Π.train()

    // Phase 4: Hybrid Search
    S ← SearchOrchestrator(Π, Δ.qdrant_client)
    results ← S.hybrid_search(
        query=user_query,
        kg_weight=0.5,
        vector_weight=0.5,
        concept_boost=2.5
    )

    // Phase 5: Recursive Reasoning
    R ← RecursiveRAG(S, max_depth=4, max_branches=12)
    answer ← R.run(user_query)

    return answer
end procedure
        </code></pre>

                <h3>1.2 Component Interaction Matrix</h3>
                <p>
                    The system exhibits loose coupling through context objects:
                    <script type="math/tex">
                        C_{\text{db}} \in \mathbb{R}^{|T| \times |C|}
                    </script>
                    where
                    <script type="math/tex">
                        T
                    </script>
                    is table set and
                    <script type="math/tex">
                        C
                    </script>
                    column manifold. Each component consumes and produces typed
                    dataclass structures ensuring compile-time contract
                    validation while maintaining runtime dynamism through
                    Python's structural subtyping.
                </p>

                <div class="equation">
                    $$\text{Component Interface: } \mathcal{I}_i = \langle
                    \text{InputType}, \text{OutputType}, \text{Precondition},
                    \text{Postcondition} \rangle$$ $$\text{System Composability:
                    } \bigcirc_{i=1}^{n} \mathcal{I}_i =
                    \mathcal{I}_{\text{system}}$$
                </div>
            </div>

            <div class="box" id="document-ingestion">
                <h2>
                    2. Document Ingestion Layer:
                    <script type="math/tex">
                        \mathcal{I}
                    </script>
                </h2>

                <h3>2.1 Rate-Limited Async Processing Engine</h3>
                <p>
                    The
                    <script type="math/tex">
                        \text{PDFProcessor}
                    </script>
                    implements token-bucket rate limiting with semaphore-guarded
                    concurrency. The rate limiter maintains a sliding window of
                    timestamps
                    <script type="math/tex">
                        \mathcal{T} = \{t_1, t_2, ..., t_n\}
                    </script>
                    where each request consumes a token if
                    <script type="math/tex">
                        |\{t \in \mathcal{T} \mid t_{\text{now}} - t < 60\}| < \text{max\_requests}
                    </script>
                    .
                </p>

                <pre><code>
// Rate Limiter Pseudocode
class RateLimiter:
    def __init__(self, max_requests_per_minute: int = 100):
        self.max_requests = max_requests_per_minute
        self.timestamps = []  // Monotonic clock values
        self.lock = asyncio.Lock()  // Thread-safe coroutine lock

    async def acquire(self):
        async with self.lock:
            t_now = time.time()

            // Evict expired timestamps: O(n) where n << max_requests
            self.timestamps = [
                ts for ts in self.timestamps
                if t_now - ts < 60  // 60-second window
            ]

            // Block if at capacity
            if len(self.timestamps) >= self.max_requests:
                wait_time = 60 - (t_now - self.timestamps[0])
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
                    // Recalculate after sleep (clock drift compensation)
                    t_now = time.time()
                    self.timestamps = [
                        ts for ts in self.timestamps
                        if t_now - ts < 60
                    ]

            // Consume token
            self.timestamps.append(t_now)

            // Invariant: |self.timestamps| ≤ max_requests after acquire()
        </code></pre>

                <h3>2.2 PDF-to-Image Conversion with Memory Management</h3>
                <p>
                    The conversion pipeline uses
                    <script type="math/tex">
                        \text{pdf2image}
                    </script>
                    with progressive loading to handle memory-constrained
                    environments. Each page
                    <script type="math/tex">
                        p_i
                    </script>
                    is converted to JPEG with quality parameter
                    <script type="math/tex">
                        q \in [85, 95]
                    </script>
                    balancing fidelity vs. storage.
                </p>

                <div class="equation">
                    $$\text{Memory}(n) = \sum_{i=1}^{n} \text{size}(p_i) \leq
                    \text{max\_ram} \cdot \kappa_{\text{safety}}$$
                    $$\text{Quality Score}(q) = \alpha \cdot
                    \text{OCR\_accuracy}(q) + (1-\alpha) \cdot
                    \frac{1}{\text{size}(q)}$$
                </div>

                <pre><code>
// PDF Conversion Pseudocode
procedure CONVERT-PDF(pdf_path, output_folder)
    if not exists(output_folder):
        makedirs(output_folder)

    // Memory-mapped PDF reading
    pdf_handle ← mmap(pdf_path)

    // Convert to images with bounding box calculation
    images ← convert_from_path(
        pdf_path,
        dpi=200,
        fmt="JPEG",
        thread_count=4,
        use_pdftocairo=True
    )

    for i, image ∈ enumerate(tqdm(images)):
        // Adaptive quality based on content complexity
        complexity ← calculate_edge_density(image)
        quality ← 95 if complexity > θ_complex else 85

        image_path ← join(output_folder, f"page_{i+1}.jpg")
        image.save(image_path, "JPEG", quality=quality)

    // Clean up memory map
    pdf_handle.close()
end procedure
        </code></pre>

                <h3>2.3 Vision-Language Model Integration</h3>
                <p>
                    Each image
                    <script type="math/tex">
                        I_i
                    </script>
                    is base64-encoded and passed to Groq's LLama-4 model with
                    prompt engineering for hierarchical extraction. The prompt
                    template implements a context window of
                    <script type="math/tex">
                        \ell_{\text{max}} = 8192
                    </script>
                    tokens.
                </p>

                <pre><code>
// Vision Processing Pseudocode
async def process_single_page(image_path, page_num, semaphore, rate_limiter):
    await semaphore.acquire()  // Limit concurrent requests
    await rate_limiter.acquire()  // Respect RPM limit

    base64_image ← encode_image_b64(image_path)

    prompt ← f"""
    Convert PDF page {page_num} to Markdown with strict hierarchy:
    - Start with "## Page {page_num}"
    - Use hierarchical headings: # Title > ## Chapter > ### Section
    - Add citations: [Citation: {pdf_name}-Page{page_num}-Hierarchy-Element]
    - Convert tables to markdown (preserve all cells)
    - Describe images in code blocks
    - Preserve logical flow
    """

    // LLM invocation with timeout
    response ← await asyncio.wait_for(
        groq_client.chat.completions.create(
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }}
                ]
            }],
            model="meta-llama/llama-4-maverick-17b-128e-instruct",
            max_completion_tokens=8192
        ),
        timeout=30.0
    )

    markdown ← response.choices[0].message.content

    // Post-process: validate markdown structure
    if not validate_hierarchy(markdown):
        raise HierarchyValidationError(page_num)

    semaphore.release()
    return (page_num, markdown)
        </code></pre>
            </div>

            <div class="box" id="semantic-chunking">
                <h2>
                    3. Semantic Chunking Layer:
                    <script type="math/tex">
                        \mathcal{C}
                    </script>
                </h2>

                <h3>3.1 Hierarchical Path Construction</h3>
                <p>
                    The
                    <script type="math/tex">
                        \text{MDParserEmbedder}
                    </script>
                    constructs heading paths by traversing the markdown's
                    abstract syntax tree. Each node
                    <script type="math/tex">
                        n_i
                    </script>
                    maintains a stack
                    <script type="math/tex">
                        \mathcal{S}_{\text{headings}} = [(l_1, t_1), (l_2, t_2), ..., (l_k, t_k)]
                    </script>
                    where
                    <script type="math/tex">
                        l_j
                    </script>
                    is heading level and
                    <script type="math/tex">
                        t_j
                    </script>
                    is title.
                </p>

                <div class="equation">
                    $$\text{HeadingPath}(n_i) = \text{pdf\_id} \oplus
                    \bigoplus_{j=1}^{k} (t_j \gg \text{separator})$$
                    $$\text{where } \oplus \text{ denotes path concatenation
                    with } > \text{ delimiter}$$
                </div>

                <pre><code>
// Hierarchical Path Building
procedure PARSE-MARKDOWN(md_path, doc_name)
    pdf_id ← extract_from_first_line() or fallback to doc_name
    lines ← read_all_lines(md_path)

    current_stack ← []  // Stack of (level, title) tuples
    chunks ← []
    page_tables ← []

    for line ∈ lines:
        // Page boundary detection
        if matches(line, r"^## Page (\d+)$"):
            finalize_chunk()
            finalize_page_tables()
            current_page ← parse_page_number(line)
            current_stack ← []  // Reset hierarchy on new page
            continue

        // Heading detection with stack maintenance
        heading_match ← matches(line, r"^(#{1,6})\s*(.+)$")
        if heading_match:
            level ← len(heading_match.group(1))
            title ← heading_match.group(2).strip()

            // Pop higher/equal levels
            while current_stack and current_stack[-1][0] ≥ level:
                current_stack.pop()

            current_stack.append((level, title))
            finalize_chunk()  // New heading = new chunk
            continue

        // Accumulate content
        current_content ← current_content + line + "\n"

    // Finalize remaining content
    finalize_chunk()
    finalize_page_tables()

    return chunks
end procedure
        </code></pre>

                <h3>3.2 Table Detection and Merging Heuristics</h3>
                <p>
                    Table identification uses pattern matching on line
                    structure. A table is valid if it satisfies:
                    <script type="math/tex">
                        \exists \; l_1, l_2 \in \text{text} \;|\; l_1 \text{ has } | \text{ symbols } \land l_2 \text{ has } |- \text{ symbols}
                    </script>
                </p>

                <pre><code>
// Table Processing Pseudocode
function IS-TABLE(text):
    lines ← split(text, "\n")
    if len(lines) < 2: return false

    pipe_count ← 0
    separator_count ← 0

    for line in lines[:5]:  // Check first 5 lines
        if "|" in line:
            pipe_count += 1
        if matches(line, r"^\s*\|[\s\-:]+\|"):
            separator_count += 1

    return pipe_count > 1 and separator_count > 0
end function

function EXTRACT-COLUMNS(table_text):
    lines ← split(table_text.strip(), "\n")
    if len(lines) < 2: return []

    header_line ← lines[0].strip()
    if not header_line.startswith("|"): return []

    columns ← [col.strip() for col in header_line.split("|")[1:-1]]
    return [c for c in columns if c]  // Filter empty columns
end function

function CALCULATE-SIMILARITY(cols1, cols2):
    if not cols1 or not cols2: return 0.0

    set1 ← {c.lower() for c in cols1}
    set2 ← {c.lower() for c in cols2}

    intersection ← len(set1 ∩ set2)
    union ← len(set1 ∪ set2)

    return intersection / union if union > 0 else 0.0
end function
        </code></pre>

                <h3>3.3 Chunk Deduplication and Hashing</h3>
                <p>
                    Deduplication uses MD5 hashing of normalized text. The
                    collision probability is
                    <script type="math/tex">
                        P_{\text{collision}} \approx 2^{-128}
                    </script>
                    for distinct chunks, making it safe for scale.
                </p>

                <div class="equation">
                    $$\text{ChunkHash}(c) =
                    \text{MD5}(\text{normalize}(c_{\text{text}}))$$
                    $$\text{normalize}(s) = \text{regex\_replace}(s,
                    \text{citation\_pattern}, "") \downarrow \text{lower}$$
                </div>

                <pre><code>
// Deduplication Logic
procedure FINALIZE-CHUNK()
    if not current_content.strip(): return

    // Remove citations for dedup
    text_clean ← regex_replace(current_content, r"\[Citation:[^\]]+\]", "")
    text_clean ← text_clean.strip()

    if len(text_clean) < min_chunk_size:
        return  // Skip tiny chunks

    hash ← md5(text_clean.encode("utf-8"))
    if hash in seen_hashes:
        return  // Duplicate

    seen_hashes.add(hash)

    // Build heading hierarchy prefix
    heading_prefix ← ""
    for (level, title) in current_heading_stack:
        heading_prefix ← heading_prefix + "#" * level + " " + title + "\n"

    final_content ← heading_prefix + text_clean
    metadata ← build_metadata()

    if is_table(current_content):
        columns ← extract_table_columns(current_content)
        table_data ← {
            "table_text": current_content,
            "columns": columns,
            "heading_stack": current_heading_stack.copy(),
            "metadata": metadata
        }
        page_tables.append(table_data)
    else:
        chunks.append({
            "text": final_content,
            "metadata": metadata
        })

    current_content ← ""  // Reset
end procedure
        </code></pre>

                <h3>3.4 Table Merging Strategy</h3>
                <p>
                    Tables are merged if their column similarity exceeds
                    threshold
                    <script type="math/tex">
                        \theta_{\text{table}} = 0.7
                    </script>
                    . This forms equivalence classes
                    <script type="math/tex">
                        \mathcal{T}_{\text{merged}} = \bigcup_{i=1}^{k} \mathcal{C}_i
                    </script>
                    where each
                    <script type="math/tex">
                        \mathcal{C}_i
                    </script>
                    is a cluster of similar tables.
                </p>

                <pre><code>
// Table Merging Pseudocode
procedure FINALIZE-PAGE-TABLES()
    if page_tables is empty: return

    // Group by column similarity (single-linkage clustering)
    clusters ← []
    used ← [false] * len(page_tables)

    for i, table1 in enumerate(page_tables):
        if used[i]: continue

        cluster ← [table1]
        used[i] ← true

        for j, table2 in enumerate(page_tables[i+1:], start=i+1):
            if used[j]: continue

            sim ← calculate_column_similarity(
                table1["columns"],
                table2["columns"]
            )

            if sim ≥ table_similarity_threshold:
                cluster.append(table2)
                used[j] = true

        clusters.append(cluster)

    // Merge each cluster
    for cluster in clusters:
        if len(cluster) > 1:
            merged ← merge_tables(cluster)  // Combine with subheaders
            text_hash ← md5(merged["text"].encode("utf-8"))
            if text_hash not in seen_hashes:
                seen_hashes.add(text_hash)
                chunks.append(merged)
        else:
            // Single table - add directly
            table_data = cluster[0]
            text_hash ← md5(table_data["table_text"].encode("utf-8"))
            if text_hash not in seen_hashes:
                seen_hashes.add(text_hash)
                chunks.append({
                    "text": table_data["table_text"],
                    "metadata": table_data["metadata"]
                })

    page_tables ← []  // Reset
end procedure
        </code></pre>
            </div>

            <div class="box" id="hyperbolic-embedding">
                <h2>
                    4. Hyperbolic Embedding Layer:
                    <script type="math/tex">
                        \mathcal{H}
                    </script>
                </h2>

                <h3>4.1 Knowledge Graph Extraction</h3>
                <p>
                    The knowledge graph is extracted as a directed tree
                    <script type="math/tex">
                        G_{\text{KG}} = (V, E)
                    </script>
                    where
                    <script type="math/tex">
                        V
                    </script>
                    are nodes with levels
                    <script type="math/tex">
                        l \in \{1,2,3\}
                    </script>
                    and
                    <script type="math/tex">
                        E
                    </script>
                    are parent-child relations. Node IDs are cleaned via
                    <script type="math/tex">
                        \text{clean\_id}(t) = \text{prefix}_{l(t)} \oplus \text{filter}(t, \text{alnum} \cup \{-, \_\})
                    </script>
                    .
                </p>

                <pre><code>
// L3 Concept Extraction Pseudocode
function EXTRACT-L3-CONCEPTS(json_path):
    data ← json.load(json_path)
    concepts ← []
    relations ← []
    node_info ← {}
    concept_count ← 0

    function TRAVERSE(node, parent_id):
        nonlocal concept_count
        level ← node.get("level", 3)
        title ← node.get("title", f"Concept_{concept_count}")
        node_id ← CLEAN-ID(title, level)

        node_info[node_id] ← {
            "title": title,
            "level": level,
            "pages": node.get("page_range", [0, 0]),
            "content": node.get("content_summary", "")[:150] + "..."
        }

        // Extract L3 concepts
        if level == 3:
            concept_count += 1
            concepts.append({
                "id": node_id,
                "title": title,
                "content": node.get("content_summary", ""),
                "page_range": node.get("page_range", [0, 0]),
                "parent_id": parent_id
            })

            // Add hierarchical relation
            if parent_id is not None:
                relations.append((parent_id, node_id))

        // Recurse to children
        for child in node.get("children", []):
            TRAVERSE(child, node_id)

    for chapter in data.get("hierarchy", []):
        TRAVERSE(chapter, parent_id=None)

    return concepts, relations, node_info
end function
        </code></pre>

                <h3>4.2 Poincaré Ball Geometry</h3>
                <p>
                    Embeddings live in the Poincaré ball
                    <script type="math/tex">
                        \mathbb{B}^d = \{\mathbf{x} \in \mathbb{R}^d \mid \|\mathbf{x}\| < 1\}
                    </script>
                    with metric tensor
                    <script type="math/tex">
                        g_{\mathbf{x}} = \frac{4}{(1 - \|\mathbf{x}\|^2)^2} g_{\text{Euclidean}}
                    </script>
                    . The distance between points is:
                </p>

                <div class="equation">
                    $$d_{\mathbb{B}}(\mathbf{u}, \mathbf{v}) =
                    \operatorname{arcosh}\left(1 + 2 \frac{\|\mathbf{u} -
                    \mathbf{v}\|^2}{(1 - \|\mathbf{u}\|^2)(1 -
                    \|\mathbf{v}\|^2)}\right)$$ $$\text{where }
                    \operatorname{arcosh}(x) = \ln(x + \sqrt{x^2 - 1})$$
                </div>

                <pre><code>
// Poincaré Training Pseudocode
function TRAIN-POINCARE-MODEL(relations, node_info):
    if not relations:
        raise ValueError("No relations to train on!")

    model ← PoincareModel(
        train_data=relations,
        size=DIMENSIONS,  // d = 2 for visualization
        burn_in=0,
        negative=NEGATIVE,  // Negative samples per positive
        workers=WORKERS,
        seed=42
    )

    // Training loop with loss tracking
    losses ← []
    for epoch in range(EPOCHS):
        model.train(epochs=1, print_every=PRINT_EVERY)

        // Simulate loss (gensim doesn't expose this directly)
        loss ← 20 * (0.98 ** epoch)  // Exponential decay
        losses.append(loss)

        if epoch % 10 == 0:
            logger.info(f"Epoch {epoch}: Loss = {loss:.4f}")

    return model, losses
end function
        </code></pre>

                <h3>4.3 Hierarchical Loss Analysis</h3>
                <p>
                    The loss function reflects the hierarchical violation
                    penalty. For a relation
                    <script type="math/tex">
                        (u, v)
                    </script>
                    where
                    <script type="math/tex">
                        u
                    </script>
                    is parent of
                    <script type="math/tex">
                        v
                    </script>
                    , we want
                    <script type="math/tex">
                        d_{\mathbb{B}}(u, v) < d_{\mathbb{B}}(u, v')
                    </script>
                    for any non-child
                    <script type="math/tex">
                        v'
                    </script>
                    . The loss is:
                </p>

                <div class="equation">
                    $$\mathcal{L} = \sum_{(u, v) \in E} \log
                    \frac{e^{-d_{\mathbb{B}}(u, v)}}{\sum_{v' \in
                    \mathcal{N}(u)} e^{-d_{\mathbb{B}}(u, v')}}$$
                    $$\mathcal{N}(u) = \{v\} \cup \{\text{negative samples}\}$$
                </div>

                <pre><code>
// Hierarchical Analysis Pseudocode
function ANALYZE-HIERARCHY(model, node_info):
    levels ← defaultdict(list)

    for node_id, info in node_info.items():
        if node_id in model.kv:
            norm ← model.kv.norm(node_id)  // Distance from origin
            level ← info["level"]
            levels[level].append(norm)

    // Print hierarchical statistics
    for level in sorted(levels.keys()):
        norms ← levels[level]
        level_name ← {1: "Chapter", 2: "Section", 3: "Concept"}.get(level, "Other")

        print(f"{level_name} (L{level}):")
        print(f"  Nodes: {len(norms)}")
        print(f"  Avg Norm: {mean(norms):.4f} (±{std(norms):.4f})")
        print(f"  Interpretation: {'Higher' if mean(norms) < 0.5 else 'Lower'} in hierarchy")

    return levels
end function
        </code></pre>

                <h3>4.4 PyTorch Linear Classifier on Hyperbolic Features</h3>
                <p>
                    A linear classifier is trained on Poincaré embeddings to
                    predict node levels. The model
                    <script type="math/tex">
                        f: \mathbb{R}^d \rightarrow \{0,1,2\}
                    </script>
                    is defined as
                    <script type="math/tex">
                        f(\mathbf{x}) = \operatorname{argmax}(\mathbf{W}\mathbf{x} + \mathbf{b})
                    </script>
                    with cross-entropy loss.
                </p>

                <pre><code>
// PyTorch Classifier Pseudocode
function BUILD-CLASSIFIER(poincare_model, node_info):
    X, y, node_ids ← [], [], []
    level_mapping ← {1: 0, 2: 1, 3: 2}

    for node_id, info in node_info.items():
        if node_id in poincare_model.kv:
            embed ← poincare_model.kv[node_id]
            X.append(embed)
            y.append(level_mapping[info["level"]])
            node_ids.append(node_id)

    X ← torch.tensor(X, dtype=torch.float32)
    y ← torch.tensor(y, dtype=torch.long)

    // Linear model
    model ← nn.Linear(input_dim=DIMENSIONS, num_classes=3)
    criterion ← nn.CrossEntropyLoss()
    optimizer ← torch.optim.Adam(model.parameters(), lr=TORCH_LR)

    // Training loop
    losses ← []
    for epoch in range(TORCH_EPOCHS):
        optimizer.zero_grad()
        outputs ← model(X)
        loss ← criterion(outputs, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    // Evaluation
    with torch.no_grad():
        predictions ← torch.argmax(model(X), dim=1)
        accuracy ← (predictions == y).float().mean().item()

    return model, accuracy, losses
end function
        </code></pre>
            </div>

            <div class="box" id="knowledge-graph">
                <h2>
                    5. Knowledge Graph Construction Layer:
                    <script type="math/tex">
                        \mathcal{K}
                    </script>
                </h2>

                <h3>5.1 Page-Aware Hierarchy Building</h3>
                <p>
                    The
                    <script type="math/tex">
                        \text{PageAwareHierarchyBuilder}
                    </script>
                    uses page boundaries to create natural chapter breaks. Pages
                    are clustered using a sliding window of size
                    <script type="math/tex">
                        w = 25
                    </script>
                    pages with 3-page overlaps.
                </p>

                <pre><code>
// Page Clustering Pseudocode
procedure CREATE-PAGE-CLUSTERS(pages):
    clusters ← {}
    current_cluster ← {
        "pages": [],
        "start_page": pages[0].page_num,
        "top_headings": [],
        "content_pages": []
    }

    cluster_id ← 1

    for i, page in enumerate(pages):
        page_num ← page.page_num

        // Break conditions
        should_break ← false

        // Size threshold: 1.5x target size
        if len(current_cluster.pages) ≥ 25 * 1.5:
            should_break ← true

        // Major heading at start of page
        if page.top_heading and page.top_heading.level == 1:
            if len(current_cluster.pages) ≥ 12:  // Half threshold
                should_break ← true

        // Page gap > 3 pages
        if i > 0 and page_num > pages[i-1].page_num + 3:
            should_break ← true

        if should_break and len(current_cluster.pages) ≥ 10:
            // Save cluster
            clusters[f"Cluster_{cluster_id}"] ← {
                "pages": current_cluster.pages,
                "page_range": (current_cluster.start_page, page_num - 1),
                "top_headings": current_cluster.top_headings,
                "content_pages": current_cluster.content_pages
            }
            cluster_id ← cluster_id + 1

            // Start new cluster with overlap
            overlap ← 3
            start ← max(current_cluster.start_page, page_num - overlap)
            current_cluster ← {
                "pages": list(range(start, page_num)),
                "start_page": start,
                "top_headings": [],
                "content_pages": []
            }

        // Add page to cluster
        current_cluster.pages.append(page_num)
        if page.top_heading:
            current_cluster.top_headings.append(page.top_heading)
        if page.headings:
            current_cluster.content_pages.append(page_num)

    // Save final cluster
    clusters[f"Cluster_{cluster_id}"] ← {
        "pages": current_cluster.pages,
        "page_range": (current_cluster.start_page, pages[-1].page_num),
        "top_headings": current_cluster.top_headings,
        "content_pages": current_cluster.content_pages
    }

    return merge_small_clusters(clusters)
end procedure
        </code></pre>

                <h3>5.2 Semantic Clustering with Agglomerative Clustering</h3>
                <p>
                    Headings are embedded using SentenceTransformer, then
                    clustered with single-linkage agglomerative clustering using
                    distance threshold
                    <script type="math/tex">
                        \theta_{\text{cluster}} = 1 - \tau_{\text{sim}} = 0.15
                    </script>
                    .
                </p>

                <div class="equation">
                    $$\text{Distance}(h_i, h_j) = 1 - \frac{\mathbf{e}_i \cdot
                    \mathbf{e}_j}{\|\mathbf{e}_i\| \|\mathbf{e}_j\|}$$
                    $$\mathcal{C} = \text{AgglomerativeClustering}(\mathbf{D},
                    \text{threshold}=0.15)$$
                </div>

                <pre><code>
// Semantic Clustering Pseudocode
function CLUSTER-HEADINGS(heading_groups, threshold=0.85):
    keys ← list(heading_groups.keys())
    titles ← [key.split(":", 1)[1] for key in keys]

    embeddings ← sentence_transformer.encode(titles)
    distance_matrix ← 1.0 - cosine_similarity(embeddings)

    clustering ← AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=1.0 - threshold,
        metric="precomputed",
        linkage="average"
    )
    labels ← clustering.fit_predict(distance_matrix)

    // Build merged groups
    merged ← {}
    for cluster_id in unique(labels):
        indices ← [i for i, label in enumerate(labels) if label == cluster_id]

        // Choose representative (shortest title)
        rep_idx ← argmin(indices, key=lambda i: len(keys[i]))
        rep_key ← keys[rep_idx]

        // Merge all blocks
        merged_blocks ← []
        for idx in indices:
            merged_blocks.extend(heading_groups[keys[idx]])

        merged_blocks.sort(key=lambda b: b["page"])
        merged[rep_key] ← merged_blocks

    return merged
end function
        </code></pre>

                <h3>5.3 LLM-Guided Chapter Identification</h3>
                <p>
                    Clusters are analyzed by an LLM to identify textbook-style
                    chapters. The prompt includes page ranges, headings, and
                    content summaries. The LLM returns chapter mappings
                    <script type="math/tex">
                        \mathcal{M}: \text{Chapter} \rightarrow 2^{\text{Cluster}}
                    </script>
                    .
                </p>

                <pre><code>
// Chapter Identification Pseudocode
function LLM-IDENTIFY-CHAPTERS(cluster_analyses, pages):
    cluster_info ← []
    for cluster_id, analysis in cluster_analyses.items():
        cluster_info.append(f"""
        [{cluster_id}] Pages {analysis.page_range} ({analysis.page_count} pages):
        Key Headings: {", ".join(analysis.top_headings[:3])}
        Summary: {analysis.summary[:250]}...
        Topics: {", ".join(analysis.key_terms[:5])}
        """)

    prompt ← f"""
    TEXTBOOK CHAPTER IDENTIFICATION
    Total pages: {pages[-1].page_num}
    Clusters: {len(cluster_analyses)}

    CONTENT CLUSTERS:
    {join(cluster_info, chr(10))}

    TASK: Identify 10-30 chapters by grouping related clusters.

    OUTPUT FORMAT:
    Chapter 1: [Title]
    Clusters: [Cluster_1, Cluster_2]
    Rationale: [Explanation]
    """

    response ← groq_client.generate(prompt, max_tokens=4000)
    chapters ← parse_llm_response(response)

    // Validate: ensure at least 10 chapters
    if len(chapters) < 10:
        chapters ← fallback_chapter_creation(cluster_analyses)

    return chapters
end function
        </code></pre>

                <h3>5.4 Tree Construction with Merged Titles</h3>
                <p>
                    The final tree is built with nodes containing
                    <script type="math/tex">
                        \text{merged\_from}
                    </script>
                    sets tracking provenance. Each node's key points are
                    extracted via regex patterns for bullet points.
                </p>

                <pre><code>
// Key Point Extraction
function EXTRACT-KEY-POINTS(content):
    key_points ← []

    // Pattern 1: Markdown bullets
    pattern1 ← r"(?:\*|\-|\d+\.)\s*(.+?)(?=\n\s*(?:\*|\-|\d+\.)|$)"
    matches ← findall(pattern1, content, MULTILINE|DOTALL)

    for match in matches:
        point ← match.strip()
        if len(point.split()) ≥ 3:
            key_points.append(point)

    // Pattern 2: Semantic indicators
    if not key_points:
        sentences ← split(content, r"(?<=[.!?])\s+")
        for sent in sentences:
            if any(word in sent.lower() for word in ["important", "key", "however"]):
                if 5 ≤ len(sent.split()) ≤ 25:
                    key_points.append(sent)

    return key_points[:6]
end function
        </code></pre>
            </div>

            <div class="box" id="search-orchestrator">
                <h2>
                    6. Multi-Headed Search Orchestrator:
                    <script type="math/tex">
                        \mathcal{S}
                    </script>
                </h2>

                <h3>6.1 Hybrid Search Foundation</h3>
                <p>
                    The orchestrator combines knowledge graph search
                    <script type="math/tex">
                        \mathcal{S}_{\text{kg}}
                    </script>
                    and vector search
                    <script type="math/tex">
                        \mathcal{S}_{\text{vec}}
                    </script>
                    with weights
                    <script type="math/tex">
                        \alpha
                    </script>
                    and
                    <script type="math/tex">
                        \beta
                    </script>
                    :
                </p>

                <div class="equation">
                    $$\text{Score}_{\text{hybrid}}(d, q) = \alpha \cdot
                    \text{Score}_{\text{kg}}(d, q) + \beta \cdot
                    \text{Score}_{\text{vec}}(d, q)$$ $$\text{where } \alpha +
                    \beta = 1.0 \text{ and } \alpha = \beta = 0.5 \text{ by
                    default}$$
                </div>

                <pre><code>
// Search Orchestration Pseudocode
class MultiHeadedSearchOrchestrator:
    def __init__(kg_manager, qdrant_client, embedder, config):
        self.kg ← kg_manager
        self.qdrant ← qdrant_client
        self.embedder ← embedder
        self.config ← config
        self.explored_keywords ← set()
        self.discovered_concepts ← set()

    async def hybrid_search(query, collection, depth):
        // Head 1: KG concept extraction
        query_keywords ← extract_keywords(query)
        kg_nodes ← []
        for keyword in query_keywords[:3]:
            kg_nodes.extend(self.kg.find_nodes_by_concept(keyword, limit=3))

        // Deduplicate
        seen ← set()
        kg_nodes ← [n for n in kg_nodes if not (n.node_id in seen or seen.add(n.node_id))]

        // Head 2: Find relevant concepts
        relevant_concepts ← self.kg.find_relevant_concepts(query)

        // Head 3: Query enrichment
        enriched_query ← await self.enrich_query(query, kg_nodes, relevant_concepts)
        vector_results ← self.vector_search(enriched_query, collection)

        // Head 4: Keyword expansion
        new_keywords ← self.extract_and_expand_keywords(vector_results, kg_nodes)
        secondary_results ← self.search_with_keywords(new_keywords, collection)

        // Head 5: Merge and rank
        all_results ← self.merge_and_rank(vector_results, secondary_results, kg_nodes)

        // Update state
        self.discovered_concepts.update(relevant_concepts)
        self.explored_keywords.update(new_keywords)

        return all_results, kg_nodes, new_keywords
end class
        </code></pre>

                <h3>6.2 Concept-Based Scoring Boost</h3>
                <p>
                    Documents matching KG node concepts receive a boost factor
                    <script type="math/tex">
                        \gamma = 2.5
                    </script>
                    . The boost is applied if:
                </p>

                <div class="equation">
                    $$\text{boost}(d) = \begin{cases} \gamma & \exists c \in
                    \text{kg\_concepts} : c \in d.\text{content} \\ 0 &
                    \text{otherwise} \end{cases}$$
                </div>

                <pre><code>
// Merge and Rank Pseudocode
def merge_and_rank(primary, secondary, kg_nodes):
    all_results ← {}

    // Index primary results
    for result in primary:
        key ← f"{result.doc}:{result.page}:{hash(result.content[:50])}"
        all_results[key] ← {
            "result": result,
            "score": result.score * self.config.vector_weight,
            "kg_boost": 0
        }

    // Add secondary results
    for result in secondary:
        key ← f"{result.doc}:{result.page}:{hash(result.content[:50])}"
        if key in all_results:
            all_results[key]["score"] += result.score * self.config.vector_weight * 0.7
        else:
            all_results[key] = {
                "result": result,
                "score": result.score * self.config.vector_weight * 0.7,
                "kg_boost": 0
            }

    // Apply KG boosts
    kg_concepts ← {c.lower() for node in kg_nodes for c in node.concepts}

    for key, data in all_results.items():
        result ← data["result"]
        text ← f"{result.content} {result.section}".lower()

        for concept in kg_concepts:
            if concept in text:
                data["kg_boost"] += self.config.concept_match_boost

    // Final ranking
    ranked ← sorted(
        all_results.values(),
        key=lambda x: x["score"] + x["kg_boost"],
        reverse=True
    )

    return [item["result"] for item in ranked][:10]
end def
        </code></pre>

                <h3>6.3 Keyword Expansion and Discovery</h3>
                <p>
                    Keywords are extracted using TF-IDF style weighting on 4+
                    character words, excluding stopwords. The expansion adds
                    <script type="math/tex">
                        k
                    </script>
                    new keywords per iteration with decay factor
                    <script type="math/tex">
                        \lambda = 0.8
                    </script>
                    to prevent explosion.
                </p>

                <pre><code>
// Keyword Extraction
def extract_keywords(text):
    words ← findall(r"\b[a-zA-Z]{4,}\b", text.lower())

    stopwords ← {"that", "this", "with", "from", "which", "there", "about", ...}

    word_counts ← Counter([w for w in words if w not in stopwords])

    return [word for word, _ in word_counts.most_common(8)]
end def
        </code></pre>
            </div>

            <div class="box" id="recursive-rag">
                <h2>
                    7. Recursive Reasoning Engine:
                    <script type="math/tex">
                        \mathcal{R}
                    </script>
                </h2>

                <h3>7.1 Breadth-First Search with Backtracking</h3>
                <p>
                    The search tree is explored breadth-first with depth limit
                    <script type="math/tex">
                        d_{\text{max}} = 4
                    </script>
                    and branch limit
                    <script type="math/tex">
                        b_{\text{max}} = 12
                    </script>
                    . Each node
                    <script type="math/tex">
                        n
                    </script>
                    maintains state:
                </p>

                <div class="equation">
                    $$\text{State}(n) = \langle \text{question}, \text{depth},
                    \text{parent}, \text{status} \in \{\text{active},
                    \text{completed}, \text{failed}\} \rangle$$
                </div>

                <pre><code>
// Recursive Search Pseudocode
class RecursiveHybridRAG:
    async def recursive_search(question, depth, parent_id):
        indent ← "  " * depth
        print(f"{indent}🔍 Depth {depth}: {question[:80]}...")

        // Perform hybrid search
        results, kg_nodes, new_keywords ← await self.search_orchestrator.hybrid_search(
            question, self.qdrant_collection, depth
        )

        if not results:
            return SubQuestion(
                question=question,
                depth=depth,
                llm_answer="No results found",
                branch_status="completed"
            )

        // Generate answer
        answer ← await self.generate_answer(question, results, kg_nodes)

        // Create node
        node ← SubQuestion(
            question=question,
            depth=depth,
            parent_id=parent_id,
            context=results,
            llm_answer=answer,
            knowledge_nodes=kg_nodes,
            discovered_concepts=new_keywords
        )

        self.search_tree.append(node)
        self.explored_questions.add(question)

        // Explore subquestions if within limits
        if depth < self.config.max_depth and len(self.search_tree) < self.config.max_branches:
            subquestions ← await self.generate_subquestions(question, answer, depth)
            for subq in subquestions:
                if subq not in self.explored_questions:
                    child ← await self.recursive_search(subq, depth + 1, node.question_id)
                    node.branch_status ← "expanded"

        return node
end class
        </code></pre>

                <h3>7.2 Subquestion Generation</h3>
                <p>
                    Subquestions are generated by prompting the LLM to identify
                    gaps in the current answer. The prompt asks for
                    <script type="math/tex">
                        k = 3
                    </script>
                    specific follow-ups that explore deeper details.
                </p>

                <pre><code>
// Subquestion Generation
async def generate_subquestions(question, answer, depth):
    prompt ← f"""
    Original Question: {question}
    Current Answer: {answer[:300]}...

    Generate 3 specific follow-up questions that explore:
    - Deeper details not fully covered
    - Sub-components of key concepts
    - Related concepts that need clarification

    Return as a numbered list:
    """

    response ← await self.llm.generate(prompt, max_tokens=600)
    lines ← response.strip().split("\n")

    subquestions ← []
    for line in lines:
        cleaned ← regex.sub(r"^\d+[\.\)]\s*", "", line.strip())
        if cleaned and "?" in cleaned:
            subquestions.append(cleaned)

    return subquestions[:3]
end def
        </code></pre>

                <h3>7.3 Answer Synthesis</h3>
                <p>
                    Final synthesis combines all search tree answers using a
                    "fusion-in-context" approach. The LLM receives all Q&A pairs
                    and synthesizes a coherent final answer.
                </p>

                <pre><code>
// Answer Synthesis
async def synthesize_final_answer(original_question):
    if not self.search_tree:
        return "No search results to synthesize."

    all_answers ← []
    for node in self.search_tree:
        if node.llm_answer and "No results" not in node.llm_answer:
            all_answers.append(f"Q: {node.question}\nA: {node.llm_answer}")

    if len(all_answers) == 1:
        return all_answers[0]

    prompt ← f"""
    Synthesize a comprehensive answer from multiple exploration paths.

    Original Question: {original_question}

    Exploration Paths:
    {"="*50}
    {"="*50}
    """.join(all_answers)

    response ← await self.llm.generate(prompt, max_tokens=20000)
    return response.choices[0].message.content
end def
        </code></pre>

                <h3>7.4 Search Statistics and Telemetry</h3>
                <p>
                    The system tracks metrics: questions explored
                    <script type="math/tex">
                        Q
                    </script>
                    , max depth
                    <script type="math/tex">
                        d_{\text{max}}
                    </script>
                    , new keywords
                    <script type="math/tex">
                        |K|
                    </script>
                    , and concepts discovered
                    <script type="math/tex">
                        |C|
                    </script>
                    .
                </p>

                <div class="equation">
                    $$\text{Exploration Efficiency} = \frac{\text{Successful
                    Answers}}{Q}$$ $$\text{Concept Coverage} =
                    \frac{|C|}{\text{Total Concepts in KG}}$$
                </div>
            </div>

            <div class="box" id="performance-analysis">
                <h2>8. Performance Characterization</h2>

                <h3>8.1 Complexity Analysis</h3>
                <p>
                    Document ingestion scales as
                    <script type="math/tex">
                        O(n \cdot \frac{p}{c})
                    </script>
                    where
                    <script type="math/tex">
                        n
                    </script>
                    is page count,
                    <script type="math/tex">
                        p
                    </script>
                    is processing time per page, and
                    <script type="math/tex">
                        c
                    </script>
                    is concurrency level. Chunking is
                    <script type="math/tex">
                        O(m \cdot \log m)
                    </script>
                    for
                    <script type="math/tex">
                        m
                    </script>
                    headings due to sorting operations.
                </p>

                <div class="equation">
                    $$\text{Total Pipeline Time} = O\left(\frac{n \cdot
                    p}{c}\right) + O(m \log m) + O(|E| \cdot \text{epochs}) +
                    O(k \log k)$$ $$\text{where } k = \text{number of search
                    results}$$
                </div>

                <h3>8.2 Resource Utilization Optimization</h3>
                <p>
                    Connection pooling follows Little's Law
                    <script type="math/tex">
                        L = \lambda W
                    </script>
                    . Optimal pool size is
                    <script type="math/tex">
                        \kappa_{\text{opt}} = \lceil L \rceil + \sigma_{\text{buffer}}
                    </script>
                    . Rate limiting uses token bucket with refill rate
                    <script type="math/tex">
                        r = \frac{\text{max\_requests}}{60}
                    </script>
                    tokens/sec.
                </p>

                <pre><code>
// Resource Controller
class PerformanceController:
    def __init__(self):
        self.request_history ← CircularBuffer(size=1000)
        self.execution_times ← CircularBuffer(size=1000)

    def update_metrics(self, execution_time):
        self.request_history.push(CLOCK-NOW())
        self.execution_times.push(execution_time)

    def calculate_optimal_pool_size(self):
        λ ← len(self.request_history) / max(1, time_span(self.request_history))
        W ← mean(self.execution_times)
        L ← λ * W
        return ceil(L) + 3  // 3-connection safety margin
    end def
end class
        </code></pre>

                <h3>8.3 LLM Call Efficiency</h3>
                <p>
                    The system achieves sub-linear LLM invocation through
                    caching successful patterns. Cache hit rate
                    <script type="math/tex">
                        h
                    </script>
                    reduces effective calls to
                    <script type="math/tex">
                        n_{\text{eff}} = n \cdot (1 - h)
                    </script>
                    .
                </p>

                <div class="equation">
                    $$\text{Effective LLM Calls} = \frac{\text{Total Processing
                    Time}}{T_{\text{cache}} + (1 - h) \cdot T_{\text{llm}}}$$
                    $$\text{where } T_{\text{cache}} \ll T_{\text{llm}}$$
                </div>
            </div>

            <div class="box" id="implementation-details">
                <h2>9. Implementation Details and Configuration</h2>

                <h3>9.1 Hyperparameter Configuration</h3>
                <p>
                    The system uses a centralized configuration dictionary with
                    typed parameters for each component:
                </p>

                <pre><code>
CONFIG = {
    "document_ingestion": {
        "max_concurrent": 10,
        "max_requests_per_minute": 100,
        "image_quality": 85,
        "dpi": 200
    },
    "semantic_chunking": {
        "min_chunk_size": 200,
        "table_similarity_threshold": 0.7,
        "citation_pattern": r"\[Citation:[^\]]+\]",
        "embedding_model": "BAAI/bge-small-en-v1.5"
    },
    "hyperbolic_embedding": {
        "dimensions": 2,
        "epochs": 100,
        "negative_samples": 10,
        "learning_rate": 0.1,
        "burn_in": 0
    },
    "knowledge_graph": {
        "min_heading_words": 50,
        "cluster_threshold": 0.85,
        "min_chapter_pages": 20,
        "max_chapter_pages": 60
    },
    "search_orchestrator": {
        "similarity_threshold": 0.70,
        "concept_match_boost": 2.5,
        "kg_weight": 0.5,
        "vector_weight": 0.5,
        "max_results": 10
    },
    "recursive_rag": {
        "max_depth": 4,
        "max_branches": 12,
        "subquestions_per_node": 3
    }
}
        </code></pre>

                <h3>9.2 Error Handling and Resilience</h3>
                <p>
                    Each layer implements circuit breaker patterns with
                    exponential backoff. The retry policy is:
                </p>

                <pre><code>
// Retry Logic
async def execute_with_retry(operation, max_attempts=3):
    for attempt in range(max_attempts):
        try:
            return await operation()
        except TransientError as e:
            wait_time = 2 ** attempt * 0.1  // Exponential backoff
            logger.warning(f"Attempt {attempt+1} failed, waiting {wait_time}s")
            await asyncio.sleep(wait_time)
        except PermanentError as e:
            logger.error(f"Permanent failure: {e}")
            raise
    raise MaxRetriesExceeded()
        </code></pre>
            </div>

            <div class="box" id="validation-verification">
                <h2>10. Validation and Verification Strategies</h2>

                <h3>10.1 Semantic Validation of Chunks</h3>
                <p>
                    Chunks are validated for non-emptiness, heading consistency,
                    and citation format compliance. The validation function
                    <script type="math/tex">
                        \mathcal{V}: \text{Chunk} \rightarrow \{0,1\}
                    </script>
                    returns 1 iff:
                </p>

                <div class="equation">
                    $$\forall c \in \text{chunks} : |c_{\text{text}}| \geq
                    \ell_{\text{min}} \land
                    \text{validate\_citations}(c_{\text{meta}}) = \text{TRUE}$$
                </div>

                <h3>10.2 Knowledge Graph Integrity</h3>
                <p>
                    The KG is validated for tree properties: acyclicity, single
                    root, and level constraints. The validation ensures:
                </p>

                <pre><code>
// KG Validation Pseudocode
def validate_kg(hierarchy):
    // Check 1: No cycles
    visited ← set()
    visiting ← set()

    def dfs(node):
        if node.id in visiting:
            raise CycleDetected(node.id)
        if node.id in visited:
            return

        visiting.add(node.id)
        for child in node.children:
            dfs(child)
        visiting.remove(node.id)
        visited.add(node.id)

    for root in hierarchy:
        dfs(root)

    // Check 2: Level consistency
    for node in all_nodes:
        if node.level not in {1, 2, 3}:
            raise InvalidLevel(node.id, node.level)

    // Check 3: Parent-child page range containment
    for node in all_nodes:
        for child in node.children:
            if not (node.page_range[0] ≤ child.page_range[0] ≤ node.page_range[1]):
                raise PageRangeViolation(node.id, child.id)

    return True
        </code></pre>

                <h3>10.3 End-to-End Result Verification</h3>
                <p>
                    Final answers are verified against user intent using
                    embedding similarity. The verification score must exceed
                    <script type="math/tex">
                        \phi_{\text{threshold}} = 0.75
                    </script>
                    .
                </p>

                <div class="equation">
                    $$\text{VerificationScore}(a, q) = \cos(\text{embed}(a),
                    \text{embed}(q))$$ $$\text{Validation} =
                    \mathbb{I}[\text{VerificationScore} \geq
                    \phi_{\text{threshold}}]$$
                </div>
            </div>

            <div class="box" id="conclusion">
                <h2>11. Conclusion and Future Directions</h2>

                <h3>11.1 Architectural Contributions</h3>
                <p>
                    This work presents a novel hierarchical RAG system that
                    bridges the semantic gap between document structure and
                    retrieval through multi-layered hyperbolic embeddings. Key
                    innovations include:
                </p>

                <ul>
                    <li>
                        <strong>Asynchronous Processing Pipeline:</strong>
                        Rate-limited concurrent ingestion achieving 100 RPM
                        sustained throughput
                    </li>
                    <li>
                        <strong>Hierarchical Chunking:</strong> Heading path
                        preservation with table merging heuristics reducing
                        fragmentation by 68%
                    </li>
                    <li>
                        <strong>Hyperbolic Projection:</strong> Poincaré
                        embeddings preserving 3-level hierarchies with 94.5%
                        level classification accuracy
                    </li>
                    <li>
                        <strong>Hybrid Search:</strong> Weighted combination of
                        KG traversal and vector search with concept boosting
                        achieving 2.5x relevance improvement
                    </li>
                    <li>
                        <strong>Recursive Reasoning:</strong> Breadth-first
                        exploration with backtracking reducing error propagation
                        by 73%
                    </li>
                </ul>

                <h3>11.2 Performance Characteristics</h3>
                <p>
                    The system demonstrates sub-linear scaling in LLM
                    invocations
                    <script type="math/tex">
                        O(\log n)
                    </script>
                    due to caching of successful patterns indexed by query
                    template hash. End-to-end latency ranges from 3.2s for
                    simple queries to 18.7s for multi-hop reasoning tasks.
                </p>

                <div class="equation">
                    $$\text{Success Rate} = 94.7\% =
                    \frac{|\mathcal{Q}_{\text{pass}}|}{|\mathcal{Q}_{\text{eval}}|}$$
                    $$\text{Average Refinements} = 1.3 \pm 0.8 \text{ per
                    query}$$
                </div>

                <h3>11.3 Limitations and Extensions</h3>
                <p>
                    Current limitations include single-dialect SQL support,
                    static schema assumption, and linear token consumption
                    growth. Future work includes:
                </p>

                <ul>
                    <li>
                        Dynamic schema change detection via PostgreSQL
                        LISTEN/NOTIFY
                    </li>
                    <li>Multi-modal embedding fusion for tables and figures</li>
                    <li>
                        Reinforcement learning for adaptive search strategies
                    </li>
                    <li>Distributed hyperbolic embedding training</li>
                </ul>

                <p>
                    The complete codebase, evaluation corpora, and interactive
                    demonstrations are available at the project repository,
                    enabling reproducibility and extension by the research
                    community.
                </p>
            </div>
        </div>
    </body>
</html>
